import streamlit as st
import yaml
import logging

from agent.data_fetcher import ResearchPaperFetcher  # optional if you want refresh feature
from agent.chunker import semantic_splitter_chunk_documents  # or your chosen chunker
from agent.vector_store import VectorStore
from agent.retriever import Retriever
from agent.agent import ResearchAgent
from data_downloader import ResearchPaperDownloader  # if using cached papers logic

# Configure logging to show in console (optional, Streamlit shows stdout/stderr)
logging.basicConfig(level=logging.INFO)

@st.cache_resource
def load_agent(config):
    # Load cached papers and build vector store only once per session
    downloader = ResearchPaperDownloader(api_url=config["semantic_scholar_api"])

    # Try load cached data
    df = downloader.load(path=config.get("cached_papers_path", "data/papers.parquet"))
    if df is None:
        st.info("No cached papers found, fetching from API...")
        df = downloader.fetch("machine learning software engineering", limit=100)
        downloader.save(df, path=config.get("cached_papers_path", "data/papers.parquet"))

    abstracts = df["abstract"].dropna().tolist()

    # Chunk the abstracts
    all_chunks = semantic_splitter_chunk_documents(
        abstracts,
        chunk_size=config['chunk_size'],
        chunk_overlap=config['chunk_overlap'],
    )

    # Initialize vector store
    vector_store = VectorStore(config["embedding_model_name"], index_path=config["faiss_index_path"])
    if vector_store.index is None:
        vector_store.create_index(all_chunks)

    retriever = Retriever(vector_store, all_chunks)
    agent = ResearchAgent(retriever, ollama_base_url=config["ollama_server_url"])

    return agent

def main():
    st.title("Research Paper AI Agent")
    st.write("""
    Enter your query below and get answers generated by the agent using  
    semantic chunked research paper data and a local LLM (Ollama Llama 3.1).
    """)

    # Load config
    with open("config/config.yaml") as f:
        config = yaml.safe_load(f)

    # Load agent (cached across reruns)
    agent = load_agent(config)

    # Text input for user query
    user_query = st.text_input("Enter your research query:", key="query_input")

    if st.button("Get Answer") and user_query.strip():
        with st.spinner("Processing your query..."):
            try:
                answer = agent.answer(user_query)
                st.markdown("### Answer:")
                st.write(answer)
            except Exception as e:
                st.error(f"Error while processing your query: {e}")

    # Optionally, provide a refresh button to update cached papers
    if st.button("Refresh Research Papers Cache"):
        with st.spinner("Refreshing cached papers... This may take a while."):
            downloader = ResearchPaperDownloader(api_url=config["semantic_scholar_api"])
            df = downloader.fetch("machine learning software engineering", limit=100)
            downloader.save(df, path=config.get("cached_papers_path", "data/papers.parquet"))
            st.success("Cache updated! Please restart the app or reload the page.")

if __name__ == "__main__":
    main()
